@article{StanfordFoundationPaper,
  author    = {Rishi Bommasani and
               Drew A. Hudson and
               Ehsan Adeli and
               Russ Altman and
               Simran Arora and
               Sydney von Arx and
               Michael S. Bernstein and
               Jeannette Bohg and
               Antoine Bosselut and
               Emma Brunskill and
               Erik Brynjolfsson and
               Shyamal Buch and
               Dallas Card and
               Rodrigo Castellon and
               Niladri S. Chatterji and
               Annie S. Chen and
               Kathleen Creel and
               Jared Quincy Davis and
               Dorottya Demszky and
               Chris Donahue and
               Moussa Doumbouya and
               Esin Durmus and
               Stefano Ermon and
               John Etchemendy and
               Kawin Ethayarajh and
               Li Fei{-}Fei and
               Chelsea Finn and
               Trevor Gale and
               Lauren Gillespie and
               Karan Goel and
               Noah D. Goodman and
               Shelby Grossman and
               Neel Guha and
               Tatsunori Hashimoto and
               Peter Henderson and
               John Hewitt and
               Daniel E. Ho and
               Jenny Hong and
               Kyle Hsu and
               Jing Huang and
               Thomas Icard and
               Saahil Jain and
               Dan Jurafsky and
               Pratyusha Kalluri and
               Siddharth Karamcheti and
               Geoff Keeling and
               Fereshte Khani and
               Omar Khattab and
               Pang Wei Koh and
               Mark S. Krass and
               Ranjay Krishna and
               Rohith Kuditipudi and
               et al.},
  title     = {On the Opportunities and Risks of Foundation Models},
  journal   = {CoRR},
  volume    = {abs/2108.07258},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.07258},
  eprinttype = {arXiv},
  eprint    = {2108.07258},
  timestamp = {Tue, 04 Jan 2022 14:40:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{skupper,
title = {Skupper.io: Let your services communicate across Kubernetes clusters},
author = {Andrea Tarocchi},
url = {https://developers.redhat.com/blog/2020/01/01/skupper-io-let-your-services-communicate-across-kubernetes-clusters},
note = {\url{https://developers.redhat.com/blog/2020/01/01/skupper-io-let-your-services-communicate-across-kubernetes-clusters}},
year = {2020},
month = {January},
day = {1}
}

@misc{TransVision2022,
author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
title = {Transformers in Vision: A Survey},
url = {https://arxiv.org/pdf/2101.01169.pdf},
note = {\url{https://arxiv.org/pdf/2101.01169.pdf}},
year = {2022}
}
%author = {Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah},

@misc{HuggingFace,
title = {Hugging Face Hub},
url = {https://huggingface.co/}
}

@book{NLPTrans2022,
Title = {Natural Language Processing with Transformers},
Author = {Lewis Tunstall, Leandro von Werra, Thomas Wolf},
year = {2022},
Publisher = {O'Reilly Media, Inc.},
ISBN = {9781098103248}
}

@inproceedings{Intriguing2021,
 author = {Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {23296--23308},
 publisher = {Curran Associates, Inc.},
 title = {Intriguing Properties of Vision Transformers},
 url = {https://proceedings.neurips.cc/paper/2021/file/c404a5adbf90e09631678b13b05d9d7a-Paper.pdf},
 volume = {34},
 year = {2021}
}

@misc{Christine2019,
author = {Payne, Christine},
title = {MuseNet},
publisher = {OpenAI}, 
year = {2019},
month ={Apr.},
day = {25},
url = {openai.com/blog/musenet},
note = {\url{openai.com/blog/musenet}}
}

@misc{BiasDistraction2022,
  doi = {10.48550/ARXIV.2203.07593},
  url = {https://arxiv.org/abs/2203.07593},
  author = {Yazdani-Jahromi, Mehdi and Rajabi, AmirArsalan and Tayebi, Aida and Garibay, Ozlem Ozmen},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Distraction is All You Need for Fairness},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{VideoUnderstanding2021,
  doi = {10.48550/ARXIV.2102.05095},
  url = {https://arxiv.org/abs/2102.05095},
  author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Is Space-Time Attention All You Need for Video Understanding?},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{WordReps2018,
  doi = {10.48550/ARXIV.1802.05365},
  url = {https://arxiv.org/abs/1802.05365},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Deep contextualized word representations},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{VideoAttention2019,
  doi = {10.48550/ARXIV.1906.02792},
  url = {https://arxiv.org/abs/1906.02792},
  author = {Bilkhu, Manjot and Wang, Siyang and Dobhal, Tushar},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{CommonSenseBERT2019,
  doi = {10.48550/ARXIV.1905.13497},
  url = {https://arxiv.org/abs/1905.13497},
  author = {Klein, Tassilo and Nabi, Moin},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is (not) All You Need for Commonsense Reasoning},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Attention2017,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{Goodfellow2016,
	title={Deep Learning},
	author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	publisher={MIT Press},
	note={\url{http://www.deeplearningbook.org}},
	year={2016}
}

@article{GPT2018,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@inproceedings{Gonzalez2012,
	author = {Joseph E. Gonzalez and Yucheng Low and Haijie Gu and Danny Bickson and Carlos Guestrin},
	title = {PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs},
	booktitle = {Presented as part of the 10th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 12)},
	year = {2012},
	isbn = {978-1-931971-96-6},
	address = {Hollywood, CA},
	pages = {17--30},
	url = {https://www.usenix.org/conference/osdi12/technical-sessions/presentation/gonzalez},
	publisher = {{USENIX}},
}

@article{PhilMarshall2017,
title = {Fast Automated Analysis of Strong Gravitational Lenses nwith Convolutional Neural Networks},
author = {Y. D. Hezaveh, L. Perreault Levasseur, and P. J. Marshall},
journal = {submitted},
year = {2017}
}

@misc{AMDandML2017,
title = {AMD Targets Machine Learning With New Radeon Vega Frontier \& Optimized Software},
author = {Karl Freund of Moor Insights and Strategy},
year = {2017},
url = {https://www.forbes.com/sites/moorinsights/2017/05/16/amd-targets-machine-learning-with-new-radeon-vega-frontier-optimized-software/#631651ba3e5b}
}

@misc{AInewLanguageFacebook2017,
title = {Facebook AI Creates Its Own Language In Creepy Preview Of Our Potential Future},
author = {Tony Bradley},
year = {2017},
journal = {Forbes},
url = {https://www.forbes.com/sites/tonybradley/2017/07/31/facebook-ai-creates-its-own-language-in-creepy-preview-of-our-potential-future/#24959fd2292c}
}

@misc{Jouppi2016,
author = {Norm Jouppi},
title = {Google supercharges machine learning tasks with TPU custom chip},
year = {2016},
journal = {Google Cloud Platform Blog},
url = {https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html}
}

@misc{AmazonFPGA,
title = {Amazon's Xilinx FPGA Cloud: Why This May Be A Significant Milestone},
author = {Karl Freund of Moor Insights and Strategy},
journal = {Forbes},
year = {2016},
url = {https://www.forbes.com/sites/moorinsights/2016/12/13/amazons-xilinx-fpga-cloud-why-this-may-be-a-significant-milestone/#3c48fb89370d}
}

@article{PhilMarshall2016,
author = {Marshall, Philip J. and Verma, Aprajita and More, Anupreeta and Davis, Christopher P. and More, Surhud and Kapadia, Amit and Parrish, Michael and Snyder, Chris and Wilcox, Julianne and Baeten, Elisabeth and Macmillan, Christine and Cornen, Claude and Baumer, Michael and Simpson, Edwin and Lintott, Chris J. and Miller, David and Paget, Edward and Simpson, Robert and Smith, Arfon M. and Küng, Rafael and Saha, Prasenjit and Collett, Thomas E.},
title = {Space Warps – I. Crowdsourcing the discovery of gravitational lenses},
journal = {Monthly Notices of the Royal Astronomical Society},
volume = {455},
number = {2},
pages = {1171-1190},
year = {2016},
doi = {10.1093/mnras/stv2009},
URL = { + http://dx.doi.org/10.1093/mnras/stv2009},
eprint = {/oup/backfile/content_public/journal/mnras/455/2/10.1093_mnras_stv2009/2/stv2009.pdf}
}

@article{Birrer2015,
	author={Simon Birrer and Adam Amara and Alexandre Refregier},
	title={Gravitational Lens Modeling with Basis Sets},
	journal={The Astrophysical Journal},
	volume={813},
	number={2},
	pages={102},
	url={http://stacks.iop.org/0004-637X/813/i=2/a=102},
	year={2015},
	abstract={We present a strong lensing modeling technique based on versatile basis sets for the lens and source planes. Our method uses high performance Monte Carlo algorithms, allows for an adaptive build up of complexity, and bridges the gap between parametric and pixel based reconstruction methods. We apply our method to a Hubble Space Telescope image of the strong lens system RX J1131-1231 and show that our method finds a reliable solution and is able to detect substructure in the lens and source planes simultaneously. Using mock data, we show that our method is sensitive to sub-clumps with masses four orders of magnitude smaller than the main lens, which corresponds to about ##IMG## [http://ej.iop.org/images/0004-637X/813/2/102/apj520927ieqn1.gif] {${10}^{8}{M}_{\odot },$} without prior knowledge of the position and mass of the sub-clump. The modeling approach is flexible and maximizes automation to facilitate the analysis of the large number of strong lensing systems expected in upcoming wide field surveys. The resulting search for dark sub-clumps in these systems, without mass-to-light priors, offers promise for probing physics beyond the standard model in the dark matter sector.}
}
